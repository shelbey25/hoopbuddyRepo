{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0ba79cf-8650-4772-bea3-28eaca2a908e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run with HoopBuddy kernel\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import io\n",
    "import pprint\n",
    "import tempfile\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from six import BytesIO\n",
    "from IPython import display\n",
    "from urllib.request import urlopen\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29df5207-3b5f-45b2-8012-a1d82c1ddc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dec8c971-b85a-42fd-a8ab-68bb9a788556",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(path):\n",
    "    model = YOLO('../models/Pose Key/weights/best.pt')\n",
    "    results = model(path)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5ae4bb3-0f38-4cee-99ea-3a61bd73df4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyPointConversion(results):\n",
    "    class_location = [[-6, 0], [0, 8], [0, -8], [12.5, 8], [12.5, -8], [11, 0], [19, 8], [19, -8]]\n",
    "    keypoints = results[0].keypoints.xy[0].tolist()\n",
    "    factor = []\n",
    "    factorAlt = []\n",
    "    for index, keypoint in enumerate(keypoints):\n",
    "        if round(keypoint[0]) != 0 or round(keypoint[1]) != 0:\n",
    "            factor = keypoint\n",
    "            factorAlt = class_location[index]\n",
    "            break\n",
    "    \n",
    "    for i in range(len(keypoints)):\n",
    "        keypoints[i] = [keypoints[i][0] - factor[0], keypoints[i][1] - factor[1]]\n",
    "        class_location[i] = [class_location[i][0] - factorAlt[0], class_location[i][1] - factorAlt[1]]\n",
    "\n",
    "    return {\"factor\": factor, \"factorAlt\": factorAlt, \"keypoints\": keypoints, \"class_location\": class_location}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "56b23873-8b0e-4842-bd47-69cd2cf628d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 37 frames to LocationPlacement/video_output/\n"
     ]
    }
   ],
   "source": [
    "def split_video_to_frames(video_path, output_dir, frames_per_second=10):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    video_capture = cv2.VideoCapture(video_path)\n",
    "    video_fps = video_capture.get(cv2.CAP_PROP_FPS)\n",
    "    frame_interval = int(video_fps // frames_per_second)\n",
    "    frame_count = 0\n",
    "    saved_frame_count = 0\n",
    "    while video_capture.isOpened():\n",
    "        ret, frame = video_capture.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if frame_count % frame_interval == 0:\n",
    "            frame_filename = os.path.join(output_dir, f'frame_{saved_frame_count:05d}.png')\n",
    "            cv2.imwrite(frame_filename, frame)\n",
    "            saved_frame_count += 1\n",
    "        frame_count += 1\n",
    "    video_capture.release()\n",
    "    print(f\"Saved {saved_frame_count} frames to {output_dir}\")\n",
    "\n",
    "#Used locally for analysis, but these paths can be changed and I did not copy paste these paths to the GitHub because they were used for expirementation\n",
    "video_path = 'LocationPlacement/166.mov'\n",
    "output_dir = 'LocationPlacement/video_output/'\n",
    "frames_per_second = 10\n",
    "\n",
    "split_video_to_frames(video_path, output_dir, frames_per_second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1622bef4-a46d-4f25-b6bd-241c1fd0c662",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images_in_directory(directory_path):\n",
    "    image_extensions = ['*.png', '*.jpg', '*.jpeg']\n",
    "    image_files = []\n",
    "    for extension in image_extensions:\n",
    "        image_files.extend(glob.glob(os.path.join(directory_path, extension)))\n",
    "    return image_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5dd9c04-4528-40f3-9af7-e5b62bc6e293",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateHomography(keypoints, class_location):\n",
    "    keypoints_fil = []\n",
    "    class_location_fil =[]\n",
    "\n",
    "    for i in range(len(keypoints)):\n",
    "        if round(keypoints[i][0]) != 0 or round(keypoints[i][1]) != 0:\n",
    "            keypoints_fil.append(keypoints[i])\n",
    "            class_location_fil.append(class_location[i])\n",
    "\n",
    "    src_pts = np.array(keypoints_fil, dtype=np.float32)\n",
    "    dst_pts = np.array(class_location_fil, dtype=np.float32)\n",
    "    H, status = cv2.findHomography(src_pts, dst_pts)\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8eafe841-0010-47fe-9e6b-68a85574a097",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_confident_coords(detections): #This is an example of a function provided by GPT, as it saved me time having to research documentation of how to parse detections\n",
    "    if detections and detections[0].boxes:  # Check if any detections exist\n",
    "        x1, y1, x2, y2 = detections[0].boxes.xyxy[0]  # Bounding box coordinates\n",
    "        confidence = detections[0].boxes.conf[0]  # Confidence score\n",
    "        x_center = (x1 + x2) / 2\n",
    "        y_center = (y1 + y2) / 2\n",
    "        return x_center, y_center, confidence\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1db6c92-f546-47d5-b40b-6ada0d587c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image_into_tiles_humans(image_path, tile_size):\n",
    "    image = Image.open(image_path)\n",
    "    image_width, image_height = image.size\n",
    "    tiles_x = image_width // tile_size\n",
    "    tiles_y = image_height // tile_size\n",
    "    tiles = []\n",
    "    for y in range(0, 2*tiles_y + 1):\n",
    "        for x in range(0, 2*tiles_x + 1):\n",
    "            left = x * (tile_size)/2\n",
    "            upper = y * (tile_size)/2\n",
    "            right = (x + 2) * (tile_size)/2\n",
    "            lower = (y + 2) * (tile_size)/2\n",
    "            tile = image.crop((left, upper, right, lower))\n",
    "            tiles.append(tile)\n",
    "            #tile.save(\"tiles/\" + image_path.split(\"/\")[-1].split(\".\")[0] + \",\" + str(x) + \",\" + str(y) + \".png\")\n",
    "    return tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "037e4fe5-66bb-4751-9ee7-f15ff48ddd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_number(path): #specifically for the given path names and the way they are named\n",
    "    match = re.search(r'(\\d+)', path)  \n",
    "    return int(match.group(1)) if match else float('inf')  \n",
    "def sort_image_paths(image_paths):\n",
    "    sorted_image_paths = sorted(image_paths, key=extract_number)\n",
    "    return sorted_image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12727908-b570-4903-902d-b5febd0b08c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_on_court(locX, locY, proccessed_results, H, scale_factor, factorAlt):\n",
    "    img_point = np.array([locX-proccessed_results[\"factor\"][0], locY-proccessed_results[\"factor\"][1]], dtype=np.float32).reshape(1, 1, 2)\n",
    "    world_point = cv2.perspectiveTransform(img_point, H)\n",
    "    #print(world_point)\n",
    "    x_new, y_new = world_point[0][0]\n",
    "    x = int(x_new*scale_factor)\n",
    "    y = int(y_new*scale_factor)\n",
    "    loc_x = x - 10 + 750 + factorAlt[0]*scale_factor\n",
    "    loc_y = -y - 10 + 750 + factorAlt[1]*scale_factor\n",
    "    if loc_x < 750+19*scale_factor and loc_y < 750+25*scale_factor and loc_y > 750-25*scale_factor:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17bfae64-02c1-4df9-af10-5e2957ab2533",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalize(convertedLocationF1, keypoints, factorAlt, H, output_dir, ball_coords): #this function is interspersed with ChatGPT snippets, specifically the initial drawing functions, but I have modified the code to fit my needs\n",
    "    players = []\n",
    "    for i in range(len(convertedLocationF1)):\n",
    "        players.append((convertedLocationF1[i][1], convertedLocationF1[i][2]))\n",
    "    #19, 8 -> 541, 133\n",
    "    #19, -8 -> 800, -16\n",
    "    #0, -8 -> 94, -88\n",
    "    #-6, 0 -> -212, -28\n",
    "    \n",
    "    def convert(inputVal):\n",
    "        img_point = np.array([inputVal[0], inputVal[1]], dtype=np.float32).reshape(1, 1, 2)\n",
    "        # Apply the homography to get the new coordinates\n",
    "        world_point = cv2.perspectiveTransform(img_point, H)\n",
    "        x_new, y_new = world_point[0][0]\n",
    "        return (x_new, y_new)\n",
    "\n",
    "    termChange = len(players)\n",
    "    for i in range(len(keypoints)):\n",
    "        players.append(convert((keypoints[i][0], keypoints[i][1])))\n",
    "    \n",
    "    # Create a white canvas (500x500) with 3 color channels (RGB)\n",
    "    canvas = np.ones((1500, 1500, 3), dtype=np.uint8) * 255  # White canvas\n",
    "    \n",
    "    # Set X marker properties\n",
    "    scale_factor = 20\n",
    "    marker_color = (0, 0, 255)  # Red color in BGR format\n",
    "    marker_thickness = 2        # Thickness of the X\n",
    "    cv2.line(canvas, (750, 750+8*scale_factor), (750+19*scale_factor, 750+8*scale_factor), marker_color, marker_thickness)\n",
    "    cv2.line(canvas, (750, 750-8*scale_factor), (750+19*scale_factor, 750-8*scale_factor), marker_color, marker_thickness)\n",
    "    cv2.line(canvas, (750, 750+8*scale_factor), (750, 750-8*scale_factor), marker_color, marker_thickness)\n",
    "    cv2.line(canvas, (0, 750+25*scale_factor), (1500, 750+25*scale_factor), marker_color, marker_thickness)\n",
    "    cv2.line(canvas, (0, 750-25*scale_factor), (1500, 750-25*scale_factor), marker_color, marker_thickness)\n",
    "    cv2.line(canvas, (750+19*scale_factor, 750-25*scale_factor), (750+19*scale_factor, 750+25*scale_factor), marker_color, marker_thickness)\n",
    "    cv2.circle(canvas, (750+15*scale_factor, 750), 10, marker_color, marker_thickness)\n",
    "    cv2.circle(canvas, (750+15*scale_factor, 750), 23*scale_factor, marker_color, marker_thickness)\n",
    "    marker_color = (255, 0, 0)\n",
    "    ball_coords_converted = convert((ball_coords[0], ball_coords[1]))\n",
    "    x = int(ball_coords_converted[0]*scale_factor)\n",
    "    y = int(ball_coords_converted[1]*scale_factor)\n",
    "    cv2.line(canvas, (x - 10 + 750 + factorAlt[0]*scale_factor, -y - 10 + 750 + factorAlt[1]*scale_factor), (x + 10 + 750 + factorAlt[0]*scale_factor, -y + 10 + 750 + factorAlt[1]*scale_factor), marker_color, marker_thickness)\n",
    "    cv2.line(canvas, (x + 10 + 750 + factorAlt[0]*scale_factor, -y - 10 + 750 + factorAlt[1]*scale_factor), (x - 10 + 750 + factorAlt[0]*scale_factor, -y + 10 + 750 + factorAlt[1]*scale_factor), marker_color, marker_thickness)\n",
    "    print((x - 10 + 750 + factorAlt[0]*scale_factor, -y - 10 + 750 + factorAlt[1]*scale_factor))\n",
    "    marker_color = (0, 0, 0) \n",
    "    \n",
    "    # Plot each player with a red 'X' on the canvas\n",
    "    for index, location in enumerate(players):\n",
    "        if (index == termChange):\n",
    "            marker_color = (0, 255, 0) \n",
    "        x = int(location[0]*scale_factor)\n",
    "        y = int(location[1]*scale_factor)\n",
    "        detected = False\n",
    "        for indexTwo, locationAlt in enumerate(players):\n",
    "            if indexTwo < termChange and indexTwo > index and location[0]*scale_factor - 30 < locationAlt[0]*scale_factor and location[0]*scale_factor + 30 > locationAlt[0]*scale_factor and location[1]*scale_factor - 30 < locationAlt[1]*scale_factor and location[1]*scale_factor + 30 > locationAlt[1]*scale_factor:\n",
    "                detected = True\n",
    "        if not detected:\n",
    "            # Draw the X marker (two diagonal lines)\n",
    "            cv2.line(canvas, (x - 10 + 750 + factorAlt[0]*scale_factor, -y - 10 + 750 + factorAlt[1]*scale_factor), (x + 10 + 750 + factorAlt[0]*scale_factor, -y + 10 + 750 + factorAlt[1]*scale_factor), marker_color, marker_thickness)\n",
    "            cv2.line(canvas, (x + 10 + 750 + factorAlt[0]*scale_factor, -y - 10 + 750 + factorAlt[1]*scale_factor), (x - 10 + 750 + factorAlt[0]*scale_factor, -y + 10 + 750 + factorAlt[1]*scale_factor), marker_color, marker_thickness)\n",
    "    \n",
    "    # Save the result as an image\n",
    "    cv2.imwrite(output_dir, canvas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f37f2cb-9034-459b-b797-ef60e41d64bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "test_mode = True\n",
    "\n",
    "def full_function():\n",
    "    model_human_pose = YOLO(\"../models/yolo11l-pose.pt\")\n",
    "    ball_model = YOLO(\"../models/Ball Detection/weights/best.pt\")\n",
    "    output_dir = \"LocationPlacement/outputs/\" #route used locally for expirementation does not work in current setup\n",
    "    image_paths = process_images_in_directory(\"LocationPlacement/video_output/\") #route used locally for expirementation does not work in current setup\n",
    "    image_paths = sort_image_paths(image_paths)\n",
    "    \n",
    "    for image_path in image_paths:\n",
    "        if (extract_number(image_path) < 7 or extract_number(image_path) > 7) and test_mode: #used to test quickly on a specific frame\n",
    "            continue\n",
    "\n",
    "        results = predict(image_path)\n",
    "        proccessed_results = keyPointConversion(results)\n",
    "        H = generateHomography(proccessed_results[\"keypoints\"], proccessed_results[\"class_location\"])\n",
    "    \n",
    "        location_of_boxes = []\n",
    "\n",
    "        tile_size = 640 \n",
    "        image = Image.open(image_path)\n",
    "        image_width, image_height = image.size\n",
    "        tiles_x = image_width // tile_size\n",
    "        tiles_y = image_height // tile_size\n",
    "        # Load the image\n",
    "        image = cv2.imread(image_path)\n",
    "        tiles = crop_image_into_tiles_humans(image_path, tile_size)\n",
    "        ballFound = False\n",
    "        ball_coords = [0, 0]\n",
    "        for index, tile in enumerate(tiles):\n",
    "            result = model_human_pose(tile)\n",
    "            for detection in result[0].keypoints:  # Assuming result[0] contains detections\n",
    "                if detection.xy != None and len(detection.xy.flatten().tolist()) >= 17:\n",
    "                    keypoints = detection.xy.flatten().tolist()  # Keypoints' x, y coordinates\n",
    "                    if (keypoints[30] != 0.0 or keypoints[31] != 0.0) or (keypoints[32] != 0.0 or keypoints[33] != 0.0):\n",
    "                        if (keypoints[30] == 0.0 and keypoints[31] == 0.0):\n",
    "                            keypoints[30] = keypoints[32]\n",
    "                            keypoints[31] = keypoints[33]\n",
    "                        if (keypoints[32] == 0.0 and keypoints[33] == 0.0):\n",
    "                            keypoints[32] = keypoints[30]\n",
    "                            keypoints[33] = keypoints[31]\n",
    "                        location_of_boxes.append([image_path, (keypoints[30]+keypoints[32])/2 + (index%(2*tiles_x + 1))*(tile_size)/2, (keypoints[31]+keypoints[33])/2 + (index//(2*tiles_x + 1))*(tile_size)/2])\n",
    "                    \n",
    "                    if (keypoints[18] != 0.0 or keypoints[19] != 0.0 or keypoints[20] != 0.0 or keypoints[21] != 0.0) and not ballFound and check_on_court((keypoints[30]+keypoints[32])/2 + (index%(2*tiles_x + 1))*(tile_size)/2, (keypoints[31]+keypoints[33])/2 + (index//(2*tiles_x + 1))*(tile_size)/2, proccessed_results, H, 20, proccessed_results[\"factorAlt\"]):\n",
    "                        if (keypoints[18] == 0.0 and keypoints[19] == 0.0):\n",
    "                            keypoints[18] = keypoints[20]\n",
    "                            keypoints[19] = keypoints[21]\n",
    "                        if (keypoints[20] == 0.0 and keypoints[21] == 0.0):\n",
    "                            keypoints[20] = keypoints[18]\n",
    "                            keypoints[21] = keypoints[19]\n",
    "\n",
    "\n",
    "                        \"\"\"\n",
    "                        This was used to estimate the ball's location by only checking tiles where a player's hand was detected, so not all tiles had to be checked\n",
    "                        While, this worked the ball detection model was not reliable enough, so this will be removed in the future, and replaced with a new way to detect the ball\n",
    "                        This code still accurately graphs people's location, however, and occasionally the ball is detected accurately as well\n",
    "                        \"\"\"\n",
    "\n",
    "                        #Keypoints no longer divided by 2\n",
    "                        coordXL = (keypoints[18]) + (index%(2*tiles_x + 1))*(tile_size)/2\n",
    "                        coordYL = (keypoints[19]) + (index//(2*tiles_x + 1))*(tile_size)/2\n",
    "                        coordXR = (keypoints[20]) + (index%(2*tiles_x + 1))*(tile_size)/2\n",
    "                        coordYR = (keypoints[21]) + (index//(2*tiles_x + 1))*(tile_size)/2\n",
    "\n",
    "                        half_check_width = 100\n",
    "                        image_dup = Image.open(image_path)\n",
    "                        image_w_ball = image_dup.crop((coordXL-half_check_width, coordYL-half_check_width, coordXL+half_check_width, coordYL+half_check_width))\n",
    "                        image_dup_2 = Image.open(image_path)\n",
    "                        image_w_ball_2 = image_dup_2.crop((coordXR-half_check_width, coordYR-half_check_width, coordXR+half_check_width, coordYR+half_check_width))\n",
    "                        \n",
    "                        detect_one = ball_model(image_w_ball)\n",
    "                        coords_one = get_most_confident_coords(detect_one)\n",
    "                        if coords_one is not None:\n",
    "                            x1, y1, confidence1 = coords_one\n",
    "                            if (confidence1 > 0.5):\n",
    "                                #ball_coords = [coordXL-half_check_width+x1, coordYL-half_check_width+y1]\n",
    "                                ball_coords = [((keypoints[30]+keypoints[32])/2 + (index%(2*tiles_x + 1))*(tile_size)/2), ((keypoints[31]+keypoints[33])/2 + (index//(2*tiles_x + 1))*(tile_size)/2)]\n",
    "                                ballFound = True\n",
    "       \n",
    "                        detect_two = ball_model(image_w_ball_2)\n",
    "                        coords_two = get_most_confident_coords(detect_two)\n",
    "                        if coords_two is not None:\n",
    "                            x2, y2, confidence2 = coords_two\n",
    "                            if (confidence2 > 0.5):\n",
    "                                #ball_coords = [coordXR-half_check_width+x2, coordYR-half_check_width+y2]\n",
    "                                ball_coords = [((keypoints[30]+keypoints[32])/2 + (index%(2*tiles_x + 1))*(tile_size)/2), ((keypoints[31]+keypoints[33])/2 + (index//(2*tiles_x + 1))*(tile_size)/2)]\n",
    "                                ballFound = True\n",
    "\n",
    "                        if test_mode and ballFound:\n",
    "                            print(coordXL)\n",
    "                            print(coordYL)\n",
    "                            print((keypoints[18]) + (index%(2*tiles_x + 1))*(tile_size)/2)\n",
    "                            print((keypoints[19]) + (index//(2*tiles_x + 1))*(tile_size)/2)\n",
    "                        \n",
    "                    \n",
    "        convertedLocation = []\n",
    "        for location in location_of_boxes:\n",
    "            img_point = np.array([location[1]-proccessed_results[\"factor\"][0], location[2]-proccessed_results[\"factor\"][1]], dtype=np.float32).reshape(1, 1, 2)\n",
    "            world_point = cv2.perspectiveTransform(img_point, H)\n",
    "            #print(world_point)\n",
    "            x_new, y_new = world_point[0][0]\n",
    "            convertedLocation.append([location[0], x_new, y_new])\n",
    "        \n",
    "        print(ball_coords)\n",
    "        ball_coords = [ball_coords[0]-proccessed_results[\"factor\"][0], ball_coords[1]-proccessed_results[\"factor\"][1]]\n",
    "        finalize(convertedLocation, proccessed_results[\"keypoints\"], proccessed_results[\"factorAlt\"], H, output_dir + image_path.split(\"/\")[-1], ball_coords)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "77980aad-0b70-4344-8caf-382e0aa47b5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /Users/shelbe/Desktop/HoopBuddy/LocationPlacement/video_output/frame_00007.png: 576x1024 1 key, 146.5ms\n",
      "Speed: 3.3ms preprocess, 146.5ms inference, 1.4ms postprocess per image at shape (1, 3, 576, 1024)\n",
      "\n",
      "0: 640x640 7 persons, 732.8ms\n",
      "Speed: 1.0ms preprocess, 732.8ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 416x416 (no detections), 52.1ms\n",
      "Speed: 0.8ms preprocess, 52.1ms inference, 0.2ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 (no detections), 56.5ms\n",
      "Speed: 1.3ms preprocess, 56.5ms inference, 0.2ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 640x640 9 persons, 608.5ms\n",
      "Speed: 2.4ms preprocess, 608.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 416x416 (no detections), 35.2ms\n",
      "Speed: 0.7ms preprocess, 35.2ms inference, 0.2ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 (no detections), 59.9ms\n",
      "Speed: 1.2ms preprocess, 59.9ms inference, 0.2ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 3 balls, 49.3ms\n",
      "Speed: 0.9ms preprocess, 49.3ms inference, 0.4ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 3 balls, 44.2ms\n",
      "Speed: 1.0ms preprocess, 44.2ms inference, 0.3ms postprocess per image at shape (1, 3, 416, 416)\n",
      "654.3938293457031\n",
      "430.70062255859375\n",
      "654.3938293457031\n",
      "430.70062255859375\n",
      "\n",
      "0: 640x640 10 persons, 615.3ms\n",
      "Speed: 1.7ms preprocess, 615.3ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 9 persons, 471.3ms\n",
      "Speed: 1.2ms preprocess, 471.3ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 4 persons, 451.4ms\n",
      "Speed: 1.0ms preprocess, 451.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 418.7ms\n",
      "Speed: 1.0ms preprocess, 418.7ms inference, 0.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 386.0ms\n",
      "Speed: 0.8ms preprocess, 386.0ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 persons, 400.4ms\n",
      "Speed: 1.1ms preprocess, 400.4ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 5 persons, 750.8ms\n",
      "Speed: 1.0ms preprocess, 750.8ms inference, 4.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 6 persons, 679.2ms\n",
      "Speed: 4.5ms preprocess, 679.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 6 persons, 408.5ms\n",
      "Speed: 0.9ms preprocess, 408.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 3 persons, 522.1ms\n",
      "Speed: 0.9ms preprocess, 522.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 498.8ms\n",
      "Speed: 1.3ms preprocess, 498.8ms inference, 0.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 392.9ms\n",
      "Speed: 1.1ms preprocess, 392.9ms inference, 0.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 395.8ms\n",
      "Speed: 0.9ms preprocess, 395.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 3 persons, 389.1ms\n",
      "Speed: 1.0ms preprocess, 389.1ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 persons, 399.7ms\n",
      "Speed: 0.7ms preprocess, 399.7ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 4 persons, 512.3ms\n",
      "Speed: 1.1ms preprocess, 512.3ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 persons, 434.5ms\n",
      "Speed: 1.1ms preprocess, 434.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 434.5ms\n",
      "Speed: 0.7ms preprocess, 434.5ms inference, 0.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 382.6ms\n",
      "Speed: 1.0ms preprocess, 382.6ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "[669.2437438964844, 568.9412841796875]\n",
      "(789, 543)\n"
     ]
    }
   ],
   "source": [
    "full_function()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hoopbuddy",
   "language": "python",
   "name": "hoopbuddy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
